#detach all loaded packages
invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload = F))

library(caret)
library(MASS)
library(dplyr)
library(randomForest)
library(kernlab)
library(corrplot)
library(GGally)
library(pROC)

diabetes0 = read.csv("https://raw.githubusercontent.com/melissaalejandro/Class-Projects/main/diabetes.csv")

##### Label 0 -> No and 1 -> Yes #####
diabetes0$Outcome[diabetes0$Outcome==0] = "No"
diabetes0$Outcome[diabetes0$Outcome==1] = "Yes"
diabetes0$Outcome = as.factor(diabetes0$Outcome)

# check for NA's
cbind(lapply(lapply(diabetes0, is.na), sum))

summary(diabetes0)

# check the number of 0's in each column
cbind(lapply(colSums(diabetes0[1:8] == 0), sum))


# keep only rows where bmi and skin thickness are >0
diabetes = diabetes0 %>% 
  filter(diabetes0$BMI > 0 & diabetes0$SkinThickness > 0)

summary(diabetes)
str(diabetes)


# ====================================================
# Data Exploration ----
# ====================================================

# Variable relationship 
ggpairs(diabetes, 
        lower = list(continuous = wrap('points', color='red')),
        diag = list(continuous = wrap("densityDiag", fill='deepskyblue')),
        upper = list(continuous = wrap('cor', color='black')))

# correlation plot
#library(corrplot)
corrplot(cor(diabetes[-9]), method="square", type = 'full', order = 'hclust', tl.col="black", tl.cex = .35, tl.srt=45, addCoef.col = 'black',number.cex=0.6)

# Boxplot Outliers 
boxplot(diabetes, col = "deepskyblue")

summary(diabetes)

# Histogram: distribution of data 
par(mfrow = c(3, 3))
for (i in 1:8) {
  hist(diabetes[ ,i], xlab = names(diabetes[i]), main = paste(names(diabetes[i]), "Histogram"), col="deepskyblue")  
}

# ====================================================
# Build the Model ----
# ====================================================

# split data 
set.seed(100)
inTrain <- createDataPartition(as.matrix(diabetes[,9]), p = .8, list=FALSE)
diab.train = diabetes[inTrain,]
diab.test = diabetes[-inTrain,]

# preprocess 
diab.TrainPP <- preProcess(diab.train, method = c("BoxCox", "scale", "center","nzv","spatialSign"))
diab.TestPP <- preProcess(diab.test, method = c("BoxCox", "scale", "center","nzv","spatialSign"))

# transform the dataset using the parameters ('predict' applies the transformations)
diabTrainTrans <- predict(diab.TrainPP, diab.train)
diabTestTrans <- predict(diab.TestPP, diab.test)

# check dist after transformation
diabetes2 = diabTrainTrans[,-9]
par(mfrow = c(3, 3))
for (i in 1:8) {
  hist(diabetes2[ ,i], xlab = names(diabetes2[i]), main = paste(names(diabetes2[i]), "Histogram"), col="deepskyblue")  
}                      

#control the computational nuances of the train function
set.seed(100)
ctrl <- trainControl(method = "LGOCV",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)



############################## 
# Logistic Regression ----
##############################
set.seed(5)
logisticTune <- train(x = as.matrix(diabTrainTrans[,1:8]), 
                      y = diabTrainTrans$Outcome,
                      method = "glm",
                      metric = "ROC",
                      trControl = ctrl)
logisticTune 

# variable importance 
plot(varImp(logisticTune, scale =FALSE))

test_results <- data.frame(obs = diabTestTrans$Outcome,
                           logistic = predict(logisticTune, diabTestTrans))


##############################
# PLSDA Regression ----
##############################
set.seed(476)
plsdaTune <- train(x = as.matrix(diabTrainTrans[,1:8]),
                   y = diabTrainTrans$Outcome,
                   method = "pls", 
                   metric = "ROC",
                   tuneGrid = expand.grid(.ncomp = 1:5),
                   trControl = ctrl)

plsdaTune
plot(plsdaTune)
plot(varImp(plsdaTune, scale =FALSE))

##############################
# SVM Radial ----
##############################
set.seed(476)
#library(kernlab)
sigmaRangeReduced <- sigest(as.matrix(diabTrainTrans[,1:8]))
svmRGridReduced <- expand.grid(.sigma = sigmaRangeReduced[1],
                               .C = 2^(seq(-4, 4)))
svmRTune <- train(x = as.matrix(diabTrainTrans[,1:8]),
                  y = diabTrainTrans$Outcome,
                  method = "svmRadial", #svmLinear #svmPoly
                  metric = "ROC",
                  preProc = c("center", "scale"),
                  tuneGrid = svmRGridReduced,
                  fit = FALSE,
                  trControl = ctrl)
svmRTune

test_results$svm <- predict(svmRTune, newdata=diabTestTrans[,-9])

plot(varImp(svmRTune, scale =FALSE))


# ====================================================
# Test Results ----
# ====================================================

##############################
# Random Forest ----
##############################

mtryGrid <- data.frame(mtry = 1:8) #since we only have 8 predictors

### Tune the model using cross-validation
set.seed(476)
rfTune <- train(x = as.matrix(diabTrainTrans[,1:8]),
                y = diabTrainTrans$Outcome,
                method = "rf",
                metric = "ROC",
                tuneGrid = mtryGrid,
                ntree = 150,
                importance = TRUE,
                trControl = ctrl)
rfTune

plot(rfTune)

plot(varImp(rfTune, scale =FALSE))

# ====================================================
# Predict the test set based on four models ---- 
# ====================================================

#logistic 
diabTestTrans$logistic <- predict(logisticTune,diabTestTrans, type = "prob")[,1]

#PLSDA
diabTestTrans$plsda <- predict(plsdaTune,diabTestTrans[,1:8], type = "prob")[,1]

#SVM
diabTestTrans$SVMR <- predict(svmRTune, diabTestTrans[,1:8], type = "prob")[,1]

#Random Forest
diabTestTrans$RF <- predict(rfTune,diabTestTrans, type = "prob")[,1]

# ====================================================
# ROC curves ----
# ====================================================

#library(pROC)
#ROC for logistic model
dev.off()
logisticROC <- roc(diabTestTrans$Outcome, diabTestTrans$logistic)
plot(logisticROC, col=1, lty=1, lwd=2)

#ROC for PLSDA
plsdaROC <- roc(diabTestTrans$Outcome, diabTestTrans$plsda)
lines(plsdaROC, col=3, lty=3, lwd=2)

#ROC for SVM
SVMROC <- roc(diabTestTrans$Outcome, diabTestTrans$SVMR)
lines(SVMROC, col=8, lty=8, lwd=2)

#ROC for Random Forest
RFROC <- roc(diabTestTrans$Outcome, diabTestTrans$RF)
lines(RFROC, col=4, lty=4, lwd=2)

legend('bottomright', c('logistic','plsda','SVM', 'Random Forest'), col=1:6, lty=1:5,lwd=2)

# ====================================================
# Create the confusion matrix from the test set ----
# ====================================================

#Confusion matrix of logistic model
a=confusionMatrix(data = predict(logisticTune, diabTestTrans), reference = diabTestTrans$Outcome)

#Confusion matrix of partial least squares discriminant analysis
b=confusionMatrix(data = predict(plsdaTune, diabTestTrans), reference = diabTestTrans$Outcome)

#Confusion matrix of SVM
c=confusionMatrix(data = predict(svmRTune, diabTestTrans[,1:8]), reference = diabTestTrans$Outcome)

#Confusion matrix of Random Forest
d=confusionMatrix(data = predict(rfTune, diabTestTrans), reference = diabTestTrans$Outcome)

# put results of confusion matrix into a df
results2 <- cbind(data.frame(logistic = c(a$overall[1:2], a$byClass[1:2])),
                 data.frame(plsda = c(b$overall[1:2], b$byClass[1:2])),
                 data.frame(svmR = c(c$overall[1:2], c$byClass[1:2])),
                 data.frame(rf = c(d$overall[1:2], d$byClass[1:2])))

# transpose df
(results2=t(results2))


#We can add the models from chapter 12
res1 = resamples(list(Logistic = logisticTune, PLSDA = plsdaTune, 
                      SVM = svmRTune, RandomForest = rfTune ))
dotplot(res1)
